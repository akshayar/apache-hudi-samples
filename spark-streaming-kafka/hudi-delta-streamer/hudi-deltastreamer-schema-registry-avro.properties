# Key fields, for kafka example
hoodie.datasource.write.recordkey.field=tradeId
hoodie.datasource.write.partitionpath.field=symbol
hoodie.datasource.write.hive_style_partitioning=true
hoodie.datasource.hive_sync.table=table_delta_streamer_avro_cow
hoodie.datasource.hive_sync.database=demohudi
hoodie.datasource.hive_sync.partition_fields=symbol
hoodie.metadata.enable=false
hoodie.index.type=GLOBAL_BLOOM
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor
#hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.NonPartitionedExtractor
# Kafka Source
hoodie.deltastreamer.source.kafka.topic=data-stream-ingest-streamer-avro
#Kafka props
bootstrap.servers=ip-10-192-11-254.ap-south-1.compute.internal:29092
auto.offset.reset=earliest
schema.registry.url=http://ip-10-192-11-254.ap-south-1.compute.internal:8081
# schema provider configs
hoodie.deltastreamer.schemaprovider.registry.url=http://ip-10-192-11-254.ap-south-1.compute.internal:8081/subjects/data-stream-ingest-streamer-avro-value/versions/latest
#hoodie.deltastreamer.filebased.schemaprovider.source.schema.file=hdfs:///TradeData.avsc
#hoodie.deltastreamer.schemaprovider.source.schema.file=hdfs:///TradeData.avsc