{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDPR Deletes\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Soft Deletes](#GDPR-Hard-Deletes)\n",
    "2. [Hard Deletes](#GDPR-Hard-Deletes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDPR has made deletes a must-have tool in everyone's data management toolbox. Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some delete operations on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 1, 'spark.dynamicAllocation.initialExecutors': 16, 'spark.sql.parquet.outputTimestampType': 'TIMESTAMP_MILLIS'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 1,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16,\n",
    "             \"spark.sql.parquet.outputTimestampType\":\"TIMESTAMP_MILLIS\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1576872917892_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-250.ec2.internal:20888/proxy/application_1576872917892_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-250.ec2.internal:8042/node/containerlogs/container_1576872917892_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "import com.google.common.collect.Lists\n",
      "import org.apache.hudi.ComplexKeyGenerator\n",
      "import org.apache.spark.sql.functions.{concat, lit}\n",
      "import org.apache.spark.sql.functions.{year, month, dayofmonth, hour}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "import com.google.common.collect.Lists;\n",
    "import org.apache.hudi.ComplexKeyGenerator\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.sql.functions.{year, month, dayofmonth, hour}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Deletes\n",
    "\n",
    "Let's pick a few records to test the soft delete functionality on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_bucket: String = hudi-workshop-100231-899011185738\n",
      "hudiTableName: String = sales_order_detail_hudi_cow\n",
      "hudiTableRecordKey: String = record_key\n",
      "hudiTablePartitionKey: String = partition_key\n",
      "hudiTablePrecombineKey: String = order_date\n",
      "hudiTablePath: String = s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_cow\n",
      "hudiHiveTablePartitionKey: String = year,month\n"
     ]
    }
   ],
   "source": [
    "//Hudi Copy on Write Table\n",
    "val s3_bucket=\"hudi-workshop-100231-899011185738\"\n",
    "val hudiTableName = \"sales_order_detail_hudi_cow\"\n",
    "val hudiTableRecordKey = \"record_key\"\n",
    "val hudiTablePartitionKey = \"partition_key\"\n",
    "val hudiTablePrecombineKey = \"order_date\"\n",
    "val hudiTablePath = s\"s3://$s3_bucket/hudi/\" + hudiTableName\n",
    "val hudiHiveTablePartitionKey = \"year,month\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a few random order_ids for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [order_id: int, quantity: int ... 1 more field]\n",
      "+--------+--------+----------+\n",
      "|order_id|quantity|order_date|\n",
      "+--------+--------+----------+\n",
      "|10001   |103     |2015-08-31|\n",
      "|10001   |118     |2015-08-31|\n",
      "|10001   |144     |2015-08-31|\n",
      "|10001   |55      |2015-08-31|\n",
      "|10001   |96      |2015-08-31|\n",
      "|10002   |77      |2015-04-25|\n",
      "|10003   |146     |2015-04-05|\n",
      "|10002   |10      |2015-04-25|\n",
      "+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df=spark.sql(\"select order_id, quantity, order_date from \"+hudiTableName+\" where order_id in (10001,10002,10003)\")\n",
    "df.show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types.IntegerType\n",
      "df: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 17 more fields]\n",
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- line_id: integer (nullable = true)\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: decimal(38,10) (nullable = true)\n",
      " |-- discount: decimal(38,10) (nullable = true)\n",
      " |-- supply_cost: decimal(38,10) (nullable = true)\n",
      " |-- tax: decimal(38,10) (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- record_key: string (nullable = true)\n",
      " |-- partition_key: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n",
      "updatedDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 17 more fields]\n",
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- line_id: integer (nullable = true)\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: decimal(38,10) (nullable = true)\n",
      " |-- discount: decimal(38,10) (nullable = true)\n",
      " |-- supply_cost: decimal(38,10) (nullable = true)\n",
      " |-- tax: decimal(38,10) (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- record_key: string (nullable = true)\n",
      " |-- partition_key: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "val df=spark.sql(\"select * from \"+hudiTableName+\" where order_id in (10001,10002,10003)\")\n",
    "df.printSchema()\n",
    "val updatedDF = df.withColumn(\"quantity\", lit(\"-1\").cast(IntegerType))\n",
    "updatedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.parquet.small.file.limit -> 67108864, hoodie.parquet.compression.ratio -> 0.5, hoodie.datasource.write.precombine.field -> order_date, hoodie.datasource.hive_sync.partition_fields -> year,month, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.parquet.max.file.size -> 1073741824, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> record_key, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> partition_key)\n"
     ]
    }
   ],
   "source": [
    "// Set up our Hudi Data Source Options\n",
    "val hudiOptions = Map[String,String](\n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> hudiTableRecordKey,\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> hudiTablePartitionKey, \n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> hudiTablePrecombineKey, \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\", \n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> hudiHiveTablePartitionKey, \n",
    "    DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\", \n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY ->\n",
    "        classOf[MultiPartKeysValueExtractor].getName,\n",
    "    \"hoodie.parquet.max.file.size\" -> String.valueOf(1024 * 1024 * 1024),\n",
    "    \"hoodie.parquet.small.file.limit\" -> String.valueOf(64 * 1024 * 1024),\n",
    "    \"hoodie.parquet.compression.ratio\" -> String.valueOf(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    " updatedDF.write \n",
    "  .format(\"org.apache.hudi\")\n",
    "  //Copy on Write Table\n",
    "  .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, DataSourceWriteOptions.COW_STORAGE_TYPE_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .option(HoodieWriteConfig.TABLE_NAME,hudiTableName)\n",
    "  .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY, hudiTableName)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now view the changed data in our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [order_id: int, quantity: int ... 1 more field]\n",
      "+--------+--------+----------+\n",
      "|order_id|quantity|order_date|\n",
      "+--------+--------+----------+\n",
      "|10002   |-1      |2015-04-25|\n",
      "|10003   |-1      |2015-04-05|\n",
      "|10002   |-1      |2015-04-25|\n",
      "|10001   |-1      |2015-08-31|\n",
      "|10001   |-1      |2015-08-31|\n",
      "|10001   |-1      |2015-08-31|\n",
      "|10001   |-1      |2015-08-31|\n",
      "|10001   |-1      |2015-08-31|\n",
      "+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df=spark.sql(\"select order_id, quantity, order_date from \"+hudiTableName+\" where order_id in (10001,10002,10003)\")\n",
    "df.show(100,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the quantity field has been updated. So essentially a soft-delete is a update where certain fields have been cleared out. You would typically do this to PII or PHI columns to anonymize the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Deletes\n",
    "\n",
    "Let's test the hard delete functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 17 more fields]\n",
      "res9: Long = 8\n"
     ]
    }
   ],
   "source": [
    "val deleteDF=spark.sql(\"select * from \"+hudiTableName+\" where order_id in (10001,10002,10003)\")\n",
    "deleteDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    " deleteDF.write\n",
    "   .format(\"org.apache.hudi\")\n",
    "   .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, DataSourceWriteOptions.COW_STORAGE_TYPE_OPT_VAL)\n",
    "   .options(hudiOptions)\n",
    "   .option(HoodieWriteConfig.TABLE_NAME,hudiTableName)\n",
    "   .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY, hudiTableName)\n",
    "   .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "   // Empty out the row with the EmptyHoodieRecordPayload\n",
    "   .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "   .mode(SaveMode.Append)\n",
    "   .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 17 more fields]\n",
      "res11: Long = 0\n"
     ]
    }
   ],
   "source": [
    "val df=spark.sql(\"select * from \"+hudiTableName+\" where order_id in (10001,10002,10003)\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the records have been deleted from our data lake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (Spark)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
