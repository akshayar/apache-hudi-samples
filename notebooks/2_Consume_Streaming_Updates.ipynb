{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume Streaming Updates to an S3 Data Lake using Apache Hudi\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Read the Rawdata](#Read-the-Rawdata)\n",
    "3. [Create Copy on Write Tables](#Create-Copy-on-Write-Tables)\n",
    "4. [Create Merge on Read Tables](#Create-Merge-on-Read-Tables)\n",
    "5. [Streaming updates to Copy on Write Tables](#Streaming-Updates-to-Copy-on-Write-Tables)\n",
    "6. [Streaming updates to Merge on Read Tables](#Streaming-Updates-to-Merge-on-Read-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to consume streaming updates to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Rawdata\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 1, 'spark.dynamicAllocation.initialExecutors': 16, 'spark.sql.parquet.outputTimestampType': 'TIMESTAMP_MILLIS'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 1,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16,\n",
    "             \"spark.sql.parquet.outputTimestampType\":\"TIMESTAMP_MILLIS\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1576872917892_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-250.ec2.internal:20888/proxy/application_1576872917892_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-250.ec2.internal:8042/node/containerlogs/container_1576872917892_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_bucket: String = hudi-workshop-100231-899011185738\n",
      "dataPath: String = s3://hudi-workshop-100231-899011185738/dms-full-load-path/salesdb/SALES_ORDER_DETAIL/LOAD*\n"
     ]
    }
   ],
   "source": [
    "val s3_bucket=\"hudi-workshop-100231-899011185738\"\n",
    "val dataPath=s\"s3://$s3_bucket/dms-full-load-path/salesdb/SALES_ORDER_DETAIL/LOAD*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read data from our SALES_ORDER_DETAIL table in the Rawdata Tier of our DataLake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [LINE_ID: int, LINE_NUMBER: int ... 8 more fields]\n",
      "df: org.apache.spark.sql.DataFrame = [line_id: int, line_number: int ... 8 more fields]\n",
      "root\n",
      " |-- line_id: integer (nullable = true)\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: decimal(38,10) (nullable = true)\n",
      " |-- discount: decimal(38,10) (nullable = true)\n",
      " |-- supply_cost: decimal(38,10) (nullable = true)\n",
      " |-- tax: decimal(38,10) (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      "\n",
      "res2: Long = 98000\n"
     ]
    }
   ],
   "source": [
    "var df=spark.read.parquet(dataPath)\n",
    "df=df.toDF(df.columns map(_.toLowerCase): _*)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "import com.google.common.collect.Lists\n",
      "import org.apache.hudi.ComplexKeyGenerator\n",
      "import org.apache.spark.sql.functions.{concat, lit}\n",
      "import org.apache.spark.sql.functions.{year, month, dayofmonth, hour}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "import com.google.common.collect.Lists;\n",
    "import org.apache.hudi.ComplexKeyGenerator\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.sql.functions.{year, month, dayofmonth, hour}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Copy on Write Tables\n",
    "\n",
    "**Copy on Write Tables** storage type stores data using exclusively columnar file formats (e.g parquet). Updates simply version & rewrite the files by performing a synchronous merge during write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiTableName: String = sales_order_detail_hudi_cow\n",
      "hudiTableRecordKey: String = record_key\n",
      "hudiTablePartitionKey: String = partition_key\n",
      "hudiTablePrecombineKey: String = order_date\n",
      "hudiTablePath: String = s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_cow\n",
      "hudiHiveTablePartitionKey: String = year,month\n",
      "inputDF: org.apache.spark.sql.DataFrame = [line_id: int, line_number: int ... 9 more fields]\n",
      "inputDF: org.apache.spark.sql.DataFrame = [line_id: int, line_number: int ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "//Hudi Copy on Write Table\n",
    "val hudiTableName = \"sales_order_detail_hudi_cow\"\n",
    "val hudiTableRecordKey = \"record_key\"\n",
    "val hudiTablePartitionKey = \"partition_key\"\n",
    "val hudiTablePrecombineKey = \"order_date\"\n",
    "val hudiTablePath = s\"s3://$s3_bucket/hudi/\" + hudiTableName\n",
    "val hudiHiveTablePartitionKey = \"year,month\"\n",
    "\n",
    "// Add Primary Key - RECORD_KEY\n",
    "var inputDF = df.withColumn(hudiTableRecordKey, concat(col(\"order_id\"), lit(\"#\"), col(\"line_id\")))\n",
    "inputDF=inputDF.select(inputDF.columns.map(x => col(x).as(x.toLowerCase)): _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some transformations to ensure that the partitions columns YEAR and MONTH are of type string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res6: org.apache.spark.sql.Row = [1,1,1,427,66,23.0000000000,0E-10,11.0000000000,1.0000000000,2015-11-11,1#1,2015,11,year=2015/month=11]\n"
     ]
    }
   ],
   "source": [
    "{ \n",
    "  import org.apache.spark.sql.types.DateType\n",
    "  import org.apache.spark.sql.types.StringType\n",
    "\n",
    "  inputDF = inputDF.withColumn(\"order_date\", inputDF(\"order_date\").cast(DateType))\n",
    "  inputDF = inputDF.withColumn(\"year\",year($\"order_date\").cast(StringType))\n",
    "    .withColumn(\"month\",month($\"order_date\").cast(StringType))\n",
    "\n",
    "  inputDF = inputDF.withColumn(hudiTablePartitionKey,concat(lit(\"year=\"),$\"year\",lit(\"/month=\"),$\"month\"))\n",
    "\n",
    "  inputDF.first()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line_id: integer (nullable = true)\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: decimal(38,10) (nullable = true)\n",
      " |-- discount: decimal(38,10) (nullable = true)\n",
      " |-- supply_cost: decimal(38,10) (nullable = true)\n",
      " |-- tax: decimal(38,10) (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- record_key: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- partition_key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the input data is prepared, let's write the data to create the Hudi COW table in the Analytics tier of our DaraLake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.parquet.small.file.limit -> 67108864, hoodie.insert.shuffle.parallelism -> 2, hoodie.parquet.compression.ratio -> 0.5, hoodie.datasource.write.precombine.field -> order_date, hoodie.datasource.hive_sync.partition_fields -> year,month, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.parquet.max.file.size -> 1073741824, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> record_key, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> partition_key)\n"
     ]
    }
   ],
   "source": [
    "// Set up our Hudi Data Source Options\n",
    "val hudiOptions = Map[String,String](\n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> hudiTableRecordKey,\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> hudiTablePartitionKey, \n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> hudiTablePrecombineKey, \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\", \n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> hudiHiveTablePartitionKey, \n",
    "    DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\", \n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY ->\n",
    "        classOf[MultiPartKeysValueExtractor].getName,\n",
    "    \"hoodie.parquet.max.file.size\" -> String.valueOf(1024 * 1024 * 1024),\n",
    "    \"hoodie.parquet.small.file.limit\" -> String.valueOf(64 * 1024 * 1024),\n",
    "    \"hoodie.parquet.compression.ratio\" -> String.valueOf(0.5),\n",
    "    \"hoodie.insert.shuffle.parallelism\" -> String.valueOf(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    " inputDF.write \n",
    "  .format(\"org.apache.hudi\")\n",
    "  //Copy on Write Table\n",
    "  .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, DataSourceWriteOptions.COW_STORAGE_TYPE_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .option(HoodieWriteConfig.TABLE_NAME,hudiTableName)\n",
    "  .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY, hudiTableName)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view and query the table created in Spark-SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CREATE EXTERNAL TABLE `sales_order_detail_hudi_cow`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `line_id` INT, `line_number` INT, `order_id` INT, `product_id` INT, `quantity` INT, `unit_price` DECIMAL(38,10), `discount` DECIMAL(38,10), `supply_cost` DECIMAL(38,10), `tax` DECIMAL(38,10), `order_date` DATE, `record_key` STRING, `partition_key` STRING)\n",
      "PARTITIONED BY (`year` STRING, `month` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_cow'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20191220210449',\n",
      "  'transient_lastDdlTime' = '1576875917'\n",
      ")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+hudiTableName).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|partition         |\n",
      "+------------------+\n",
      "|year=2015/month=1 |\n",
      "|year=2015/month=10|\n",
      "|year=2015/month=11|\n",
      "|year=2015/month=12|\n",
      "|year=2015/month=2 |\n",
      "|year=2015/month=3 |\n",
      "|year=2015/month=4 |\n",
      "|year=2015/month=5 |\n",
      "|year=2015/month=6 |\n",
      "|year=2015/month=7 |\n",
      "|year=2015/month=8 |\n",
      "|year=2015/month=9 |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions \"+hudiTableName).show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|98000   |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+hudiTableName).show(100,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Merge on Read Tables\n",
    "\n",
    "\n",
    "**Merge on Read** storage type enables clients to  ingest data quickly into a row based data format such as Avro. Any new data that is written to the Hudi dataset using MOR table type, will write new log/delta files that internally store the data as Avro encoded bytes. \n",
    "\n",
    "A compaction process (configured as inline or asynchronous) will convert the log file format to the columnar base file format (parquet). The two different InputFormats expose 2 different views of this data:\n",
    "\n",
    "- Read Optimized view exposes columnar parquet reading performance \n",
    "- Realtime View exposes columnar and/or log reading performance respectively. \n",
    "\n",
    "Updating an existing set of rows will result in either \n",
    "\n",
    "* a) a companion log/delta file for an existing base parquet file generated from a previous compaction or \n",
    "* b) an update written to a log/delta file in case no compaction ever happened for it. Hence, all writes to such datasets are limited by avro/log file writing performance, much faster than parquet. Although, there is a higher cost to pay to read log/delta files vs columnar (parquet) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiTableName: String = sales_order_detail_hudi_mor\n",
      "hudiTablePath: String = s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_mor\n"
     ]
    }
   ],
   "source": [
    "//Hudi Merge On Read Table\n",
    "val hudiTableName = \"sales_order_detail_hudi_mor\"\n",
    "val hudiTablePath = s\"s3://$s3_bucket/hudi/\" + hudiTableName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The write operation to create the MOR Storage Type table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    " inputDF.write \n",
    "  .format(\"org.apache.hudi\")\n",
    "  // Merge on Read Table this time.  \n",
    "  .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, DataSourceWriteOptions.MOR_STORAGE_TYPE_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .option(HoodieWriteConfig.TABLE_NAME,hudiTableName)\n",
    "  .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY, hudiTableName)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CREATE EXTERNAL TABLE `sales_order_detail_hudi_mor`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `line_id` INT, `line_number` INT, `order_id` INT, `product_id` INT, `quantity` INT, `unit_price` DECIMAL(38,10), `discount` DECIMAL(38,10), `supply_cost` DECIMAL(38,10), `tax` DECIMAL(38,10), `order_date` DATE, `record_key` STRING, `partition_key` STRING)\n",
      "PARTITIONED BY (`year` STRING, `month` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_mor'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20191220210537',\n",
      "  'transient_lastDdlTime' = '1576875951'\n",
      ")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+hudiTableName).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|partition         |\n",
      "+------------------+\n",
      "|year=2015/month=1 |\n",
      "|year=2015/month=10|\n",
      "|year=2015/month=11|\n",
      "|year=2015/month=12|\n",
      "|year=2015/month=2 |\n",
      "|year=2015/month=3 |\n",
      "|year=2015/month=4 |\n",
      "|year=2015/month=5 |\n",
      "|year=2015/month=6 |\n",
      "|year=2015/month=7 |\n",
      "|year=2015/month=8 |\n",
      "|year=2015/month=9 |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions \"+hudiTableName).show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|98000   |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+hudiTableName).show(100,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the tables from Presto and Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can SSH to the Spark Presto cluster from the Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "$> cd SageMaker\n",
    "$> chmod 400 ee-default-keypair.pem\n",
    "$> ssh -i ee-default-keypair.pem hadoop@ec2-54-80-95-22.compute-1.amazonaws.com\n",
    "$> presto-cli\n",
    "```\n",
    "\n",
    "and query the data:\n",
    "\n",
    "```mysql\n",
    "presto> use hive.default;\n",
    "presto> show tables;\n",
    "presto> select count(*) from sales_order_detail_hudi_cow;\n",
    "presto> select count(*) from sales_order_detail_hudi_mor;\n",
    "```\n",
    "\n",
    "Press Ctrl+D to exist Presto-cli, and run the following command to run hive.\n",
    "\n",
    "```bash\n",
    "$> hive\n",
    "```\n",
    "\n",
    "```mysql\n",
    "# view the tables\n",
    "hive> show tables;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Make sure to run a Kernel->Shutdown on this notebook before the next steps. This will free up resources on the Spark EMR Cluster for the next steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Updates to Copy on Write Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The following step need to be executed from the terminal available within Jupyter. Please start the Simulate Random Updates step in the 1st notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSH to the Spark EMR cluster from the Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "$> cd SageMaker\n",
    "$> chmod 400 ee-default-keypair.pem\n",
    "$> ssh -i ee-default-keypair.pem hadoop@ec2-54-158-247-127.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "Run the following command in the terminal once ssh-ed into the EMR cluster:\n",
    "\n",
    "\n",
    "```bash\n",
    "EMR $> spark-submit --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --conf \"spark.dynamicAllocation.maxExecutors=10\" --jars hdfs:///hudi-spark-bundle.jar --jars hdfs:///spark-avro.jar --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.2 --class com.hudiConsumer.SparkKafkaConsumerHudiProcessor_COW SparkKafkaConsumerHudiProcessor-assembly-1.0.jar hudi-workshop-100231-899011185738 b-2.kafkacluster1.zf8cl7.c6.kafka.us-east-1.amazonaws.com:9094,b-1.kafkacluster1.zf8cl7.c6.kafka.us-east-1.amazonaws.com:9094 \n",
    "```\n",
    "\n",
    "This command launches a Spark Streaming job that continuously monitors the Kafka topic 's3_event_streams' to consume the updates into the Hudi table 'sales_order_detail_hudi_cow' in S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the changing data\n",
    "\n",
    "You can SSH to the Spark Presto cluster from the Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "$> cd SageMaker\n",
    "$> ssh -i ee-default-keypair.pem hadoop@ec2-54-80-95-22.compute-1.amazonaws.com\n",
    "$> presto-cli\n",
    "```\n",
    "\n",
    "and query the data:\n",
    "```mysql\n",
    "presto> use hive.default;\n",
    "presto> show tables;\n",
    "\n",
    "# pick one record key here\n",
    "presto> select record_key,quantity,month from sales_order_detail_hudi_cow where record_key = '<record_key>';\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please press Ctrl+C in the terminal connected to the EMR Spark Cluster to stop the Spark Streaming job.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Updates to Merge on Read Tables\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The following step need to be executed from the terminal available within Jupyter. Make sure you have the Database Streaming Updates step running in the other notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSH to the Spark EMR cluster from the Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "ssh -i ee-default-keypair.pem hadoop@ec2-54-158-247-127.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "Run the following command in the terminal once ssh-ed into the EMR cluster:\n",
    "\n",
    "```bash\n",
    "spark-submit --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --conf \"spark.dynamicAllocation.maxExecutors=10\" --jars hdfs:///hudi-spark-bundle.jar --jars hdfs:///spark-avro.jar --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.2  --class com.hudiConsumer.SparkKafkaConsumerHudiProcessor_MOR SparkKafkaConsumerHudiProcessor-assembly-1.0.jar hudi-workshop-100231-899011185738 b-2.kafkacluster1.zf8cl7.c6.kafka.us-east-1.amazonaws.com:9094,b-1.kafkacluster1.zf8cl7.c6.kafka.us-east-1.amazonaws.com:9094 \n",
    "```\n",
    "The command launches a Spark Streaming job that continuously monitors the Kafka topic 's3_event_streams' to consume the updates into the Hudi table 'sales_order_detail_hudi_mor' in S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the changing data\n",
    "\n",
    "You can SSH to the Spark Presto cluster from the Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "ssh -i ee-default-keypair.pem hadoop@ec2-54-80-95-22.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "and query the data but this time we are going to use **Hive** to run our queries:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace the <b>record_key</b> argument value in the next command with a record_key from the Spark Streaming logs.\n",
    "</div>\n",
    "\n",
    "```mysql\n",
    "$> hive\n",
    "# view the tables\n",
    "hive> show tables;\n",
    "# pick one record key here\n",
    "hive> select record_key,quantity,month from sales_order_detail_hudi_mor where record_key = '<record_key>'; \n",
    "# let's query the same record in the realtime table\n",
    "hive> select record_key,quantity,month from sales_order_detail_hudi_mor_rt where record_key = '<record_key>'; \n",
    "```\n",
    "\n",
    "We can observe that the Realtime table has the latest view of changes that is not yet compacted into our main MOR base table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please press Ctrl+C in the terminal connected to the EMR Spark Cluster to stop the Spark Streaming job.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Hudi Compaction Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the **Apache Hudi Compaction** process manually so that we understand the behavior. These steps will typically be automated in a production environment.\n",
    "\n",
    "```bash\n",
    "## From the terminal, let's connect to the Spark EMR cluster and start the hudi cli\n",
    "$> /usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "## at the hudi cli, let's connect to the datapath for the MOR table \n",
    "hudi-> connect --path s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_mor\n",
    "\n",
    "## run a describe on this table\n",
    "hudi:sales_order_detail_hudi_mor-> desc\n",
    "\n",
    "## view the pending commits\n",
    "hudi:sales_order_detail_hudi_mor-> commits show\n",
    "\n",
    "## schedule the compactions on the table\n",
    "hudi:sales_order_detail_hudi_mor-> compaction schedule\n",
    "\n",
    "## refresh hudi metadata after compaction schedule is successful\n",
    "hudi-> connect --path s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_mor\n",
    "\n",
    "## view the pending compactions\n",
    "hudi:sales_order_detail_hudi_mor-> compactions show all\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace the <b>compactionInstant</b> argument value in the next command as obtained from the previous command.\n",
    "</div>\n",
    "\n",
    "```bash\n",
    "## execute the compactions on the table\n",
    "hudi:sales_order_detail_hudi_mor-> compaction run --parallelism 12 --sparkMemory 100GB --retry 1 --compactionInstant <compactionInstant> --schemaFilePath s3://hudi-workshop-100231-899011185738/config/sales_order_detail.schema\n",
    "\n",
    "## refresh hudi metadata after compactions run is successful\n",
    "hudi-> connect --path s3://hudi-workshop-100231-899011185738/hudi/sales_order_detail_hudi_mor\n",
    "\n",
    "## view the completed compactions\n",
    "hudi:sales_order_detail_hudi_mor-> compactions show all\n",
    "\n",
    "## the compactions should show Completed now. Let's view the commits and query the  changes in Hive as well:\n",
    "hudi:sales_order_detail_hudi_mor-> commits show\n",
    "\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace the <b>latest commit id</b> argument value in the next command as obtained from the previous command.\n",
    "</div>\n",
    "\n",
    "\n",
    "```bash\n",
    "## Let's now rollback the latest commit.\n",
    "hudi:sales_order_detail_hudi_mor-> commit rollback --commit <latest commit id>\n",
    "\n",
    "## After the step completes, lets view the commits again and query the changes in Hive as well:\n",
    "hudi:sales_order_detail_hudi_mor-> commits show\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the compacted 'sales_order_detail_hudi_mor' table to view the latest changes. Let's do that from Hive in our Presto EMR Cluster:\n",
    "\n",
    "```bash\n",
    "## start the hive cli\n",
    "$> hive\n",
    "```\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace with the same <b>record_key</b> as used above.\n",
    "</div>\n",
    "\n",
    "```mysql\n",
    "## query the changed record\n",
    "hive> select record_key,quantity,month from sales_order_detail_hudi_mor where record_key = '<record_key>';\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (Spark)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
