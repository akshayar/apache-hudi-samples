{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo : Batch Updates to a S3 Datalake using Apache Hudi\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "3. [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "4. [Deleting Records](#Deleting-Records.)\n",
    "5. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)\n",
    "6. [Creating Manifests for Athena/Redshift Spectrum](#Creating_Manifests_for_Athena/Redshift_Spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**This demo notebook runs fine on a single node (r5.4xlarge) EMR Cluster version 5.30.**\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to upsert records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "- Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS. \n",
    "- When working with the AWS Glue Catalog, the httpclient-4.5.9.jar library is needed to be the 1st jar specified in the spark.jars configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T21:57:09.103690Z",
     "start_time": "2020-04-25T21:57:09.084257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///httpclient-4.5.9.jar,hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar,', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 1, 'spark.dynamicAllocation.initialExecutors': 16}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///httpclient-4.5.9.jar,hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar,\",\n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 1,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T21:58:41.043305Z",
     "start_time": "2020-04-25T21:58:41.014388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1590604325881_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-38-163.us-west-2.compute.internal:20888/proxy/application_1590604325881_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-163.us-west-2.compute.internal:8042/node/containerlogs/container_1590604325881_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"example_hudi_table\",\n",
    "    \"target\": \"s3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_table\",\n",
    "    \"primary_key\": \"id\",\n",
    "    \"sort_key\": \"sk\",\n",
    "    \"commits_to_retain\": \"2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T21:58:44.095168Z",
     "start_time": "2020-04-25T21:58:44.063412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:00:26.220770Z",
     "start_time": "2020-04-25T22:00:26.195389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    now = str(datetime.today().replace(microsecond=0))\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"txt\": chr(65 + (i % 26)), \"modified_time\" : now} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 4M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T21:28:05.962035Z",
     "start_time": "2020-04-25T21:27:44.650881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- modified_time: string (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      "\n",
      "+---+-------------------+---+---+\n",
      "| id|      modified_time| sk|txt|\n",
      "+---+-------------------+---+---+\n",
      "|  0|2020-05-27 20:48:57|  0|  A|\n",
      "|  1|2020-05-27 20:48:57|  1|  B|\n",
      "|  2|2020-05-27 20:48:57|  2|  C|\n",
      "+---+-------------------+---+---+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "df1 = create_json_df(spark, get_json_data(0, 4000000))\n",
    "print(df1.count())\n",
    "df1.printSchema()\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------------+\n",
      "|id |sk |txt|modified_timestamp |\n",
      "+---+---+---+-------------------+\n",
      "|0  |0  |A  |2020-05-27 20:48:57|\n",
      "|1  |1  |B  |2020-05-27 20:48:57|\n",
      "|2  |2  |C  |2020-05-27 20:48:57|\n",
      "+---+---+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "df2=df1.withColumn(\"modified_timestamp\",F.to_timestamp(F.col('modified_time'), \"yyyy-MM-dd HH:mm:ss\")).drop(\"modified_time\")\n",
    "df2.show(3,False)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(id,LongType,true),StructField(modified_time,StringType,true),StructField(sk,LongType,true),StructField(txt,StringType,true)))"
     ]
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:42:04.365176Z",
     "start_time": "2020-04-25T20:40:44.746489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:42:22.544723Z",
     "start_time": "2020-04-25T20:42:21.820549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE .hoodie/\r\n",
      "2020-05-27 20:49:37          0 .hoodie_$folder$\r\n",
      "2020-05-27 20:50:17         93 .hoodie_partition_metadata\r\n",
      "2020-05-27 20:50:30    9867126 3812e9ba-9376-4ea1-a61c-38a6be5b9645-0_1-9-13_20200527204934.parquet\r\n",
      "2020-05-27 20:50:31   10704243 aa492e8d-e0e1-4e39-a01c-07cdf9ad4e6c-0_2-9-14_20200527204934.parquet\r\n",
      "2020-05-27 20:50:35   13949105 ea2cdfe9-cf72-4652-8199-22a05ac3fb32-0_0-9-12_20200527204934.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!aws s3 ls  s3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "\n",
    "4M records took approximately 1min 20s to get written to S3 in 3 files using a single node EMR cluster - r4.2xlarge.\n",
    "\n",
    "When sizing an EMR Cluster, when the the number of executors matches the number of files to be written, you get perfect parallelism. However if the number of files being written is too large, the number of executors will be far smaller than the number of files to be written which will be the more usual scenario.\n",
    "\n",
    "Some Apache Hudi Write Parameters to note when performing a bulk-insert are below though you may not need to override them all for every workload:\n",
    "\n",
    "| Storage configs | ProbDistribution |\n",
    "| :--- | :--- | \n",
    "| hoodie.parquet.max.file.size | Target size for parquet files produced by Hudi write phases. |\n",
    "| hoodie.parquet.small.file.limit | This should be less < maxFileSize. |\n",
    "| hoodie.parquet.compression.ratio | Expected compression of parquet data used by Hudi. |\n",
    "| hoodie.bulkinsert.shuffle.parallelism | Parallelism determines the initial number of files in your table. |\n",
    "\n",
    "Let's inspect the table created and query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:42:31.457294Z",
     "start_time": "2020-04-25T20:42:26.188018Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:42:51.231679Z",
     "start_time": "2020-04-25T20:42:50.488882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `example_hudi_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `id` BIGINT, `sk` BIGINT, `txt` STRING, `modified_timestamp` BIGINT)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_table'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1590612643',\n",
      "  'last_commit_time_sync' = '20200527204934'\n",
      ")\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extra columns that are added by Hudi to keep track of commits and filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:43:03.413201Z",
     "start_time": "2020-04-25T20:42:58.146075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:43:11.447778Z",
     "start_time": "2020-04-25T20:43:06.168903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|4000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T21:59:14.695861Z",
     "start_time": "2020-04-25T21:58:53.329855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|id     |sk     |\n",
      "+-------+-------+\n",
      "|3000000|3000000|\n",
      "|3000001|3000001|\n",
      "|3000002|3000002|\n",
      "|3000003|3000003|\n",
      "|3000004|3000004|\n",
      "|3000005|3000005|\n",
      "|3000006|3000006|\n",
      "|3000007|3000007|\n",
      "|3000008|3000008|\n",
      "|3000009|3000009|\n",
      "|3000010|3000010|\n",
      "+-------+-------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, sk from \"+config['table_name'] +\" where id between 3000000 and 3000010\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:00:48.514680Z",
     "start_time": "2020-04-25T22:00:46.263430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000"
     ]
    }
   ],
   "source": [
    "df2 = create_json_df(spark, get_json_data(3000000, 10000, 2))\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- modified_time: string (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+-------------------+\n",
      "|id     |sk     |txt|modified_timestamp |\n",
      "+-------+-------+---+-------------------+\n",
      "|3000000|3000002|Q  |2020-05-27 20:51:01|\n",
      "|3000001|3000003|R  |2020-05-27 20:51:01|\n",
      "|3000002|3000004|S  |2020-05-27 20:51:01|\n",
      "+-------+-------+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "df3=df2.withColumn(\"modified_timestamp\",F.to_timestamp(F.col('modified_time'), \"yyyy-MM-dd HH:mm:ss\")).drop(\"modified_time\")\n",
    "df3.show(3,False)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:00:49.228653Z",
     "start_time": "2020-04-25T22:00:48.993091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|     id|     sk|\n",
      "+-------+-------+\n",
      "|3000000|3000002|\n",
      "|3000001|3000003|\n",
      "|3000002|3000004|\n",
      "|3000003|3000005|\n",
      "|3000004|3000006|\n",
      "+-------+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df3.select(\"id\",\"sk\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have incremented the value in the sk column by 1 for those 10 records and let's write the changes to S3. Note that the operation now is Upsert as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:01:34.158698Z",
     "start_time": "2020-04-25T22:00:52.740947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:01:37.780200Z",
     "start_time": "2020-04-25T22:01:37.047331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE .hoodie/\r\n",
      "2020-05-27 20:49:37          0 .hoodie_$folder$\r\n",
      "2020-05-27 20:50:17         93 .hoodie_partition_metadata\r\n",
      "2020-05-27 20:51:29    9867588 3812e9ba-9376-4ea1-a61c-38a6be5b9645-0_0-52-355_20200527205105.parquet\r\n",
      "2020-05-27 20:50:30    9867126 3812e9ba-9376-4ea1-a61c-38a6be5b9645-0_1-9-13_20200527204934.parquet\r\n",
      "2020-05-27 20:50:31   10704243 aa492e8d-e0e1-4e39-a01c-07cdf9ad4e6c-0_2-9-14_20200527204934.parquet\r\n",
      "2020-05-27 20:50:35   13949105 ea2cdfe9-cf72-4652-8199-22a05ac3fb32-0_0-9-12_20200527204934.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!aws s3 ls  s3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected : 4 files as one file has now 2 versions stored.\n",
    "\n",
    "Let's rerun the previous cell and observe the number of files. \n",
    "\n",
    "And once more and observe the number of files. Expected : 5 files.\n",
    "\n",
    "Notice that the number of files is not increasing beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy is deleting older files when writing as the commits_to_retain policy is set to 2. \n",
    "\n",
    "Let's query our changed files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:08.784575Z",
     "start_time": "2020-04-25T20:44:07.530381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:14.607095Z",
     "start_time": "2020-04-25T20:44:11.347983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|id     |sk     |modified_timestamp|\n",
      "+-------+-------+------------------+\n",
      "|2000000|2000000|1590612537000000  |\n",
      "|2000001|2000001|1590612537000000  |\n",
      "|2000002|2000002|1590612537000000  |\n",
      "|2000003|2000003|1590612537000000  |\n",
      "|2000004|2000004|1590612537000000  |\n",
      "|2000005|2000005|1590612537000000  |\n",
      "|2000006|2000006|1590612537000000  |\n",
      "|2000007|2000007|1590612537000000  |\n",
      "|2000008|2000008|1590612537000000  |\n",
      "|2000009|2000009|1590612537000000  |\n",
      "|2000010|2000010|1590612537000000  |\n",
      "+-------+-------+------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, sk, modified_timestamp from \"+config['table_name'] +\" where id between 2000000 and 2000010\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:18.158615Z",
     "start_time": "2020-04-25T20:44:17.419288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "## Generates Data\n",
    "def get_json_data(start, count, increment=0):\n",
    "    now = str(datetime.today().replace(microsecond=0))\n",
    "    data = [{\"id\": i} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "df2 = create_json_df(spark, get_json_data(2000000, 10, 1))\n",
    "df2.createOrReplaceTempView(\"data_to_delete_v\")\n",
    "# join the incoming delete ids with the original table.\n",
    "df3=spark.sql(\"SELECT a.* from \"+config['table_name']+' a,data_to_delete_v b where a.id = b.id')\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      " |-- modified_timestamp: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now delete these 10 records. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.common.model.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:42.460928Z",
     "start_time": "2020-04-25T20:44:21.117184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:51.009531Z",
     "start_time": "2020-04-25T20:44:45.748549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|id |sk |\n",
      "+---+---+\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, sk from \"+config['table_name'] +\" where id between 2000000 and 2000009\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records no longer exist in our table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:55.515239Z",
     "start_time": "2020-04-25T20:44:55.487709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"example_hudi_partitioned_table\",\n",
    "    \"target\": \"s3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_partitioned_table\",\n",
    "    \"primary_key\": \"id\",\n",
    "    \"sort_key\": \"sk\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"year,month\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:44:58.076336Z",
     "start_time": "2020-04-25T20:44:58.049608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "## Generates Data - we are adding year and month columns this time.\n",
    "def get_json_data(start, count, increment=0):\n",
    "    data = [{\"id\": i, \"sk\": i+increment, \"txt\": chr(65 + (i % 26)), \"year\" : \"2019\", \"month\": random.randint(1,12) } for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:45:32.416261Z",
     "start_time": "2020-04-25T20:45:00.333022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000"
     ]
    }
   ],
   "source": [
    "df1 = create_json_df(spark, get_json_data(0, 4000000))\n",
    "print(df1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe. Note that we are doing this because we want Hive Style Partitions : year=<year>/month=<month> etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:45:33.155404Z",
     "start_time": "2020-04-25T20:45:32.418356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     partitionKey|\n",
      "+-----------------+\n",
      "|year=2019/month=3|\n",
      "|year=2019/month=4|\n",
      "|year=2019/month=6|\n",
      "|year=2019/month=7|\n",
      "|year=2019/month=1|\n",
      "+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"partitionKey\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"year=\"),col(\"year\"),lit(\"/month=\"),col(\"month\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"partitionKey\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:47:57.635342Z",
     "start_time": "2020-04-25T20:46:19.912571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"partitionKey\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:48:05.491810Z",
     "start_time": "2020-04-25T20:48:04.736925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `example_hudi_partitioned_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `id` BIGINT, `sk` BIGINT, `txt` STRING, `partitionkey` STRING)\n",
      "PARTITIONED BY (`year` STRING, `month` BIGINT)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://[YOUR-S3-BUCKET]/tmp/hudi/example_hudi_partitioned_table'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20200527205243',\n",
      "  'transient_lastDdlTime' = '1587847672'\n",
      ")\n",
      "|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table. \n",
    "\n",
    "```\n",
    "PARTITIONED BY (`year` STRING, `month` BIGINT)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T20:48:16.231776Z",
     "start_time": "2020-04-25T20:48:08.953259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+\n",
      "|year|month|count(1)|\n",
      "+----+-----+--------+\n",
      "|2019|1    |334289  |\n",
      "|2019|2    |332567  |\n",
      "|2019|3    |333834  |\n",
      "|2019|4    |332667  |\n",
      "|2019|5    |333940  |\n",
      "|2019|6    |333554  |\n",
      "|2019|7    |333280  |\n",
      "|2019|8    |332993  |\n",
      "|2019|9    |332970  |\n",
      "|2019|10   |333269  |\n",
      "|2019|11   |333607  |\n",
      "|2019|12   |333030  |\n",
      "+----+-----+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select year, month, count(*) from \"+config['table_name']+\" group by year, month order by month\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
