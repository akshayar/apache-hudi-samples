{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Athena SymlinkTextInputFormat Tables Over Apache Hudi Tables <a name=\"top\"></a>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Create or Update the Athena Table](#Create-or-Update-the-Athena-Table)\n",
    "2. [Create or Update the symlink text files](#Create-or-Update-the-symlink-text-files)\n",
    "3. [Load All the Partitions](#Load-All-the-Partitions)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note : </b>This notebook is tested on Apache Hudi 0.5.2-incubating on Amazon EMR 5.30</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Amazon Athena does not natively support Apache Hudi tables today. But as Apache Hudi tables are in Parquet, if we point Amazon Athena to the latest files in our Apache Hudi dataset, then Amazon Athena should be able to read them.\n",
    "\n",
    "In this notebook, we demonstrate how to create SymlinkTextInputFormat tables to point Amazon Athena to the latest Apache Hudi files. This will allow Amazon Athena to query the the latest version of Parquet files in our Apache Hudi Dataset.\n",
    "\n",
    "Note that the same Parquet table can be queried by Amazon Redshift Spectrum or any other engine that supports reading symlinkformat tables and Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:43:44.711808Z",
     "start_time": "2020-06-08T20:43:44.678504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///httpclient-4.5.9.jar,hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar,'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>24</td><td>application_1591058751782_0044</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-38-211.us-west-2.compute.internal:20888/proxy/application_1591058751782_0044/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-211.us-west-2.compute.internal:8042/node/containerlogs/container_1591058751782_0044_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///httpclient-4.5.9.jar,hdfs:///hudi-spark-bundle.jar,hdfs:///spark-avro.jar,\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:08.482172Z",
     "start_time": "2020-06-08T20:43:48.201544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>26</td><td>application_1591058751782_0046</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-38-211.us-west-2.compute.internal:20888/proxy/application_1591058751782_0046/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-211.us-west-2.compute.internal:8042/node/containerlogs/container_1591058751782_0046_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.hudi.hive.HiveSyncConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "import collection.JavaConverters._\n",
      "import org.apache.hadoop.hive.conf.HiveConf\n",
      "import org.apache.hudi.hive.HoodieHiveClient\n",
      "import org.apache.hadoop.fs.Path\n",
      "import org.apache.hudi.common.util.Option\n",
      "import org.apache.hudi.hive.util.SchemaUtil\n",
      "import org.apache.hudi.common.table.HoodieTableMetaClient\n",
      "import org.apache.hudi.common.util.FSUtils\n",
      "import org.apache.hudi.common.model.HoodieCommitMetadata\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n",
      "import org.apache.hudi.table.HoodieTable\n",
      "import org.apache.spark.SerializableWritable\n"
     ]
    }
   ],
   "source": [
    "import org.apache.hudi.hive.HiveSyncConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "import collection.JavaConverters._\n",
    "import org.apache.hadoop.hive.conf.HiveConf\n",
    "import org.apache.hudi.hive.HoodieHiveClient\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.hudi.common.util.Option\n",
    "import org.apache.hudi.hive.util.SchemaUtil\n",
    "import org.apache.hudi.common.table.HoodieTableMetaClient\n",
    "import org.apache.hudi.common.util.FSUtils\n",
    "import org.apache.hudi.common.model.HoodieCommitMetadata\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n",
    "import org.apache.hudi.table.HoodieTable\n",
    "import org.apache.spark.SerializableWritable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:14.912582Z",
     "start_time": "2020-06-08T20:44:13.673449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tablePath: String = s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table\n",
      "hiveTablePartitionFieldsList: String = year,month\n",
      "hiveSyncConfig: org.apache.hudi.hive.HiveSyncConfig = HiveSyncConfig{databaseName='null', tableName='null', hiveUser='null', hivePass='null', jdbcUrl='null', basePath='null', partitionFields=[], partitionValueExtractorClass='org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor', assumeDatePartitioning=false, usePreApacheInputFormat=false, useJdbc=true, help=false}\n",
      "hiveSyncConfig.basePath: String = s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table\n",
      "hiveSyncConfig.jdbcUrl: String = jdbc:hive2://localhost:10000\n",
      "hiveSyncConfig.databaseName: String = default\n",
      "hiveSyncConfig.tableName: String = example_hudi_partitioned_table\n",
      "athena_tableName: String = example_hudi_partitioned_table_athena\n"
     ]
    }
   ],
   "source": [
    "val tablePath = \"s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table\"\n",
    "\n",
    "val hiveTablePartitionFieldsList = \"year,month\"\n",
    "val hiveSyncConfig = new HiveSyncConfig()\n",
    "hiveSyncConfig.basePath = tablePath\n",
    "hiveSyncConfig.jdbcUrl = \"jdbc:hive2://localhost:10000\"\n",
    "hiveSyncConfig.databaseName = \"default\"\n",
    "hiveSyncConfig.tableName = \"example_hudi_partitioned_table\"\n",
    "val athena_tableName=hiveSyncConfig.tableName+\"_athena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:24.647285Z",
     "start_time": "2020-06-08T20:44:17.382751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiveSyncConfig.partitionFields: java.util.List[String] = [year, month]\n",
      "hiveSyncConfig.partitionValueExtractorClass: String = org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml\n",
      "metaClient: org.apache.hudi.common.table.HoodieTableMetaClient = HoodieTableMetaClient{basePath='s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table', metaPath='s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table/.hoodie', tableType=COPY_ON_WRITE}\n",
      "hiveConfig: org.apache.hadoop.hive.conf.HiveConf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@7b8c6e50, file:/etc/spark/conf.dist/hive-site.xml\n",
      "hoodieHiveClient: org.apache.hudi.hive.HoodieHiveClient = org.apache.hudi.hive.HoodieHiveClient@6867d2de\n",
      "dataSchema: org.apache.parquet.schema.MessageType =\n",
      "message hoodie.example_hudi_partitioned_table.example_hudi_partitioned_table_record {\n",
      "  optional binary _hoodie_commit_time (UTF8);\n",
      "  optional binary _hoodie_commit_seqno (UTF8);\n",
      "  optional binary _hoodie_record_key (UTF8);\n",
      "  optional binary _hoodie_partition_path (UTF8);\n",
      "  optional binary _hoodie_file_name (UTF8);\n",
      "  optional int64 id;\n",
      "  optional int64 month;\n",
      "  optional int64 sk;\n",
      "  optional binary txt (UTF8);\n",
      "  optional binary year (UTF8);\n",
      "  optional binary partitionKey (UTF8);\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "hiveSyncConfig.partitionFields =  hiveTablePartitionFieldsList.split(\",\").map(_.trim).filter(!_.isEmpty).toList.asJava\n",
    "hiveSyncConfig.partitionValueExtractorClass = classOf[MultiPartKeysValueExtractor].getName\n",
    "\n",
    "var hadoopConf = spark.sparkContext.hadoopConfiguration\n",
    "val metaClient = new HoodieTableMetaClient(hadoopConf, tablePath, true)\n",
    "\n",
    "val hiveConfig = new HiveConf()\n",
    "hiveConfig.addResource(metaClient.getFs.getConf)\n",
    "\n",
    "val hoodieHiveClient = new HoodieHiveClient(hiveSyncConfig, hiveConfig, metaClient.getFs)\n",
    "val dataSchema = hoodieHiveClient.getDataSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Update the Athena Table\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Let's create the Athena Parquet table or alter the table if the schema has changed. Our original table is called `example_hudi_partitioned_table` and we have decided to call the corresponding SymlinkTextInputFormat table `example_hudi_partitioned_table_athena`.\n",
    "\n",
    "We will create a `.athena` folder within the S3 folder for the original table to create the Partitions and Symlink files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:26.897999Z",
     "start_time": "2020-06-08T20:44:24.649786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE  IF NOT EXISTS `default`.`example_hudi_partitioned_table_athena`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `id` bigint, `sk` bigint, `txt` string, `partitionKey` string) PARTITIONED BY (`year` string,`month` bigint) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat' LOCATION 's3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table'\n",
      "ALTER TABLE `default.example_hudi_partitioned_table_athena` SET LOCATION \"s3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table/.athena\"\n"
     ]
    }
   ],
   "source": [
    "if (!hoodieHiveClient.doesTableExist(athena_tableName))\n",
    "{\n",
    "  val serDeClassName = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
    "  val inputFormatClassName =  \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\"\n",
    "  val outputFormatClassName = \"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\"\n",
    "\n",
    "  val createTableDDL = SchemaUtil.generateCreateDDL(athena_tableName, dataSchema, hiveSyncConfig, inputFormatClassName,outputFormatClassName,serDeClassName)\n",
    "  println(createTableDDL)  \n",
    "  hoodieHiveClient.updateHiveSQL(createTableDDL)\n",
    "  val alterTableDDL = new StringBuilder(\"ALTER TABLE \").append(\"`\").append(hiveSyncConfig.databaseName)\n",
    "    .append(\".\").append(athena_tableName).append(\"`\").append(\" SET LOCATION \\\"\").append(hiveSyncConfig.basePath)\n",
    "    .append(\"/.athena\\\"\")\n",
    "  println(alterTableDDL.toString())\n",
    "  hoodieHiveClient.updateHiveSQL(alterTableDDL.toString())\n",
    "} else{\n",
    "  val tableSchema = hoodieHiveClient.getTableSchema(athena_tableName)\n",
    "  val schemaDiff = SchemaUtil.getSchemaDifference(dataSchema, tableSchema, hiveSyncConfig.partitionFields)\n",
    "  if (!schemaDiff.isEmpty)\n",
    "  {\n",
    "    val newSchemaStr = SchemaUtil.generateSchemaString(dataSchema, hiveSyncConfig.partitionFields)\n",
    "    val alterTableDDL = new StringBuilder(\"ALTER TABLE \").append(\"`\").append(hiveSyncConfig.databaseName)\n",
    "      .append(\".\").append(athena_tableName).append(\"`\").append(\" REPLACE COLUMNS(\").append(newSchemaStr)\n",
    "      .append(\" )\")\n",
    "    hoodieHiveClient.updateHiveSQL(alterTableDDL.toString())\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Update the symlink text files\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Next we will create or update the symlink.txt files. We will first read the partitions and then drop the symlink.txt into the partition folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:31.304309Z",
     "start_time": "2020-06-08T20:44:30.065840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activeTimeline: org.apache.hudi.common.table.timeline.HoodieActiveTimeline = org.apache.hudi.common.table.timeline.HoodieActiveTimeline: [20200527222733__clean__COMPLETED],[20200527222733__commit__COMPLETED]\n",
      "lastCommit: org.apache.hudi.common.table.timeline.HoodieInstant = [20200527222733__commit__COMPLETED]\n",
      "writerConfig: org.apache.hudi.config.HoodieWriteConfig = org.apache.hudi.config.HoodieWriteConfig@75d184a8\n",
      "hoodieTable: org.apache.hudi.table.HoodieTable[Nothing] = org.apache.hudi.table.HoodieCopyOnWriteTable@74e55f30\n",
      "serializedConf: org.apache.spark.SerializableWritable[org.apache.hadoop.conf.Configuration] = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml\n"
     ]
    }
   ],
   "source": [
    "val activeTimeline = metaClient.getActiveTimeline\n",
    "val lastCommit = activeTimeline.getCommitTimeline.lastInstant.get\n",
    "val writerConfig = HoodieWriteConfig.newBuilder().withPath(tablePath).build()\n",
    "val hoodieTable = HoodieTable.getHoodieTable(metaClient, writerConfig, spark.sparkContext)\n",
    "val serializedConf = new SerializableWritable(hadoopConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:36.301475Z",
     "start_time": "2020-06-08T20:44:33.986392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitionPaths: java.util.List[String] = [year=2019/month=1, year=2019/month=10, year=2019/month=11, year=2019/month=12, year=2019/month=2, year=2019/month=3, year=2019/month=4, year=2019/month=5, year=2019/month=6, year=2019/month=7, year=2019/month=8, year=2019/month=9]\n",
      "allPartitionPaths: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:44\n",
      "year=2019/month=1\n",
      "year=2019/month=10\n",
      "year=2019/month=11\n",
      "year=2019/month=12\n",
      "year=2019/month=2\n",
      "year=2019/month=3\n",
      "year=2019/month=4\n",
      "year=2019/month=5\n",
      "year=2019/month=6\n",
      "year=2019/month=7\n"
     ]
    }
   ],
   "source": [
    "val partitionPaths=FSUtils.getAllPartitionPaths(metaClient.getFs,tablePath,false)\n",
    "val allPartitionPaths = sc.parallelize(partitionPaths.asScala)\n",
    "allPartitionPaths.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the partitions, lets create the 'symlink.txt' files into the .athena folder. We will parallelize the problem using Apache Spark by assigning the work to the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:42.354029Z",
     "start_time": "2020-06-08T20:44:39.103039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoopConf: org.apache.hadoop.conf.Configuration = null\n",
      "res8: Array[Unit] = Array((), (), (), (), (), (), (), (), (), (), (), ())\n"
     ]
    }
   ],
   "source": [
    "hadoopConf=null\n",
    "allPartitionPaths.map(partitionPath => {\n",
    "  val view = hoodieTable.getBaseFileOnlyView\n",
    "  val latestFiles = view.getLatestBaseFilesBeforeOrOn(partitionPath, lastCommit.toString).iterator.asScala.map( s => s.getPath).toList\n",
    "  val dest = new Path(tablePath + \"/.athena/\" + partitionPath + \"/symlink.txt\")\n",
    "  val fs1 = dest.getFileSystem(serializedConf.value)\n",
    "  val out =  fs1.create(dest, true)\n",
    "  out.write(latestFiles.mkString(\"\\n\").getBytes)\n",
    "  out.close()\n",
    "}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All the Partitions\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Next we will load the partitions to the Metastore using MSCK REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T20:44:47.572245Z",
     "start_time": "2020-06-08T20:44:45.330446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msckRepairTableDDL: StringBuilder = MSCK REPAIR TABLE `default.example_hudi_partitioned_table_athena`\n"
     ]
    }
   ],
   "source": [
    "val msckRepairTableDDL = new StringBuilder(\"MSCK REPAIR TABLE \").append(\"`\").append(hiveSyncConfig.databaseName).append(\".\").append(athena_tableName).append(\"`\")\n",
    "hoodieHiveClient.updateHiveSQL(msckRepairTableDDL.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to query the `example_hudi_partitioned_table_athena` in Amazon Athena.\n",
    "\n",
    "If we run a ```SHOW CREATE TABLE `example_hudi_partitioned_table_athena````, we can see that it is using SymlinkTextInputFormat as the INPUTFORMAT and ParquetHiveSerDe as the SERDE.\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE `example_hudi_partitioned_table_athena`(\n",
    "  `_hoodie_commit_time` string, \n",
    "  `_hoodie_commit_seqno` string, \n",
    "  `_hoodie_record_key` string, \n",
    "  `_hoodie_partition_path` string, \n",
    "  `_hoodie_file_name` string, \n",
    "  `id` bigint, \n",
    "  `sk` bigint, \n",
    "  `txt` string, \n",
    "  `partitionkey` string)\n",
    "PARTITIONED BY ( \n",
    "  `year` string, \n",
    "  `month` bigint)\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "  's3://neilawstmp2/tmp/hudi/example_hudi_partitioned_table/.athena'\n",
    "TBLPROPERTIES (\n",
    "  'last_modified_by'='anonymous', \n",
    "  'last_modified_time'='1591648275')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (Spark)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
